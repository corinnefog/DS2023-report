---
title: "DS2023 Report"
type: book
---

**Jolie Ng, Gracie Williams, Ruth Melese, Corinne Fogarty**

DS2023

Group EDA

04/16/2025

# Breaking Down the Game: Exploratory Data Analysis of Basketball On-Court Data

## Loading in the data

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
from google.colab import files
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 128}
# load in the two data sets
uploaded = files.upload()
uploaded = files.upload()

df1 = pd.read_csv('catapult season 1.csv')
df2 = pd.read_csv('catapult season 2.csv')
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
# look at the first 5 rows to see the columns and entry formats
display(df1.head())
display(df2.head())
```

# Step 1: Who is our stakeholder?
It is no secret that the 2024 season was much less successful in terms of wins and losses than the 2023 season. Instead of looking at outside factors like a change in the head coach and changes to the starting line-up, our team wants to look at data concerning player load and effort between the practices leading up to games and the games themselves. By looking at this data, we will be able to deduce if there are any correlations between the success of the season and physical effort from players. Our discoveries will hopefully be able to inform the coaching staff on the best ways for players to regulate their energy expenditure to maximize performance on the court. Our main research questions is: Based on the player workload and movement metrics amongst the 2023 and 2024 season, what trends can inform training decisions and injury prevention strategies for the UVA Strength and Conditioning staff?

# Step 2: What is our problem statement?

Based on the player workload and movement metrics amongst the 2023 and 2024 season, what trends can inform training decisions and injury prevention strategies for the UVA Strength and Conditioning staff?

By “player workload and movement metrics,” we refer to variables such as Player Load, Inertial Movement Analysis (IMA), jump counts, and changes of direction (CoDs), which together provide insight into the physical demands placed on athletes during training and competition.

# Step 3: What are the important variables?

The key variables we wanted to look particularly at are Total Player Load, Player Load Per Mintue, Position, Explosive Efforts, and Acceleration Efforts.

First, lets make a data dictionary so that we can have a key of all the variables, and what they all mean.

```{python}
#| colab: {base_uri: https://localhost:8080/}
data_dictionary = {
    "Position": "The player’s role on the team (e.g., Guard, Forward, Center), which influences movement patterns and workload.",
    "Period Number": "The specific segment of the game or practice session being recorded.",
    "Total Player Load": "A cumulative measure of the physical workload experienced by a player during a session.",
    "Player Load Per Minute": "Player load normalized per minute to account for differences in playing time.",
    "IMA Accel Total": "The total number of inertial movement accelerations across all intensity levels.",
    "IMA Decel Total": "The total number of inertial movement decelerations across all intensity levels.",
    "Explosive Efforts": "The number of high-intensity movements, such as quick sprints or jumps.",
    "Session Total Jump": "The total number of jumps performed by a player during the session.",
    "Session Jumps Per Minute": "The number of jumps a player performs per minute, useful for workload analysis.",
    "Total High IMA": "The total number of high-intensity inertial movement activities, capturing rapid changes in motion."
}

print("My Data Dictionary for Key Variables - what do they mean?\n")
for key, value in data_dictionary.items():
    print(f"{key}: {value}")
```

#Step 4: Understanding and Cleaning the Data



Our first step in tackling these two datasets is just to understand the data we’re working with. We need to look at the structures of the datasets (including their shapes, variable types, missing values, etc.) and get a feel for the different variables. But first, let's determine if there is any missing data or duplicate entries.

```{python}
#| colab: {base_uri: https://localhost:8080/}
num_duplicates1 = df1.duplicated().sum()
num_duplicates2 = df2.duplicated().sum()
print("\nNumber of duplicate rows in season 1 data:", num_duplicates2)
print("\nNumber of duplicate rows in season 2 data:", num_duplicates2)
```

No duplicate entries - great! Now let's look for missing values.

```{python}
#| colab: {base_uri: https://localhost:8080/}
missing_values1 = df1.isnull().sum()
print(missing_values1)

print("\n")

missing_values2 = df2.isnull().sum()
print(missing_values2)
```

We can also see the missing values visually:

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
import missingno as msno

plt.figure(figsize=(15, 10))
msno.bar(df1)
plt.title("Season 1 No. of Entries by Variable")
plt.show()

print("\n")

plt.figure(figsize=(15, 10))

msno.bar(df2)
plt.title("Season 2 No. of Entries by Variable")
plt.show()
```

We can see that the session data variables and some other variables (e.g. total IMA, IMA/Min, etc.) have large amounts of missing data compared to the actual number of data entries. This means if/when we analyze those variables, we have to create a new dataframe or change our original one to only include the entries with that specific variable filled in to do any sort of analysis.

Now let's look at the season 1 data first:

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
display(df1.info())  # gives us the data types and non-null counts
print("\nShape of Season 1 Dataset:", df1.shape)  # shows us number of rows and columns
```

Then, we can do the same for season 2 data:

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 1000}
display(df2.info())  # gives us the data types and non-null counts
print("\nShape of Season 1 Dataset:", df2.shape)  # shows us number of rows and columns
```

As we can see, this dataset has many different types of predictors, but mostly float and integer variables  (numbers).

---

Now we want to identify the most valuable predictors (MVPs), so we need to separate all the variables into two groups - categorical and numerical - to more easiy see which variables will be the most interesting and useful to explore in our analysis.

For Season 1 Data:

```{python}
#| colab: {base_uri: https://localhost:8080/}
categorical_vars1 = df1.select_dtypes(include=['object']).columns.tolist()
numerical_vars1 = df1.select_dtypes(include=['float64', 'int64']).columns.tolist()
print("\nCategorical variables:\n", "\n".join(categorical_vars1), "\n")
print("Numerical variables:\n", "\n".join(numerical_vars1))
```

For Season 2 Data:

```{python}
#| colab: {base_uri: https://localhost:8080/}
categorical_vars2 = df2.select_dtypes(include=['object']).columns.tolist()
numerical_vars2 = df2.select_dtypes(include=['float64', 'int64']).columns.tolist()
print("\nCategorical variables:\n", "\n".join(categorical_vars2), "\n")
print("Numerical variables:\n", "\n".join(numerical_vars2))
```

The variables in the two datasets are the same.

Let's make sure that the dates are in standard datetime format. Currently they are in MM/DD/YYYY.

```{python}
#| colab: {base_uri: https://localhost:8080/}
# making sure the dates are in standard datetime format
df1['Date'] = pd.to_datetime(df1['Date'], format='%m/%d/%y', errors='coerce')
df2['Date'] = pd.to_datetime(df2['Date'], format='%m/%d/%y', errors='coerce')

df1 = df1.sort_values('Date')
df2 = df2.sort_values('Date')

# print a date to check
print(df1['Date'].head())
print(df2['Date'].head())
```

# Step 5: Descriptive Statistics & Distributions

Now we want to look at the numerical data values from the two datasets separately and look at their summary statistics.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 765}
display(df1[numerical_vars1].describe())
display(df2[numerical_vars2].describe())
```

Now we want to see which variables will be the most interesting and useful to explore in our analysis. We will use a correlation matrix with all of the numerical variables.

# Step 6 & Step 7: Examine Correlations & Explore Relationships

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 867}
plt.figure(figsize=(30, 15))
correlation_matrix1 = df1[numerical_vars1].corr()
sns.heatmap(correlation_matrix1, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Matrix for Numerical Values of Season 1 Data")
plt.show()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 867}
plt.figure(figsize=(30, 15))
correlation_matrix2 = df2[numerical_vars2].corr()
sns.heatmap(correlation_matrix2, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Matrix for Numerical Values of Season 2 Data")
plt.show()
```

Just from a brief look, there loos to be a relationship between Total Player Load vs Explosive Efforts. Those are both relatively summarizing statistics compared to the rest and they also have a decently positive but not perfect correlation (0.73) - perfect for exploring! Referencing our missing data analysis above, both Total Player Load and Explosive Efforts do not have any missing entries and the entries are all in numerical form, so we can proceed with graphing.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 612}
key_metrics = [
    'Total Player Load',
    'Player Load Per Minute',
    'Explosive Efforts',
    'Total Acceleration Efforts',
    'IMA CoD Left Low',
    'IMA CoD Right Low',
    'IMA CoD Left Medium',
    'IMA CoD Right Medium',
    'IMA CoD Left High',
    'IMA CoD Right High'
]

sns.heatmap(df2[key_metrics].corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation: Key Workload Metrics", loc='center')
plt.show()
```

From looking at this heat map above, we see strong correlations in Total Player Load, Explosive Efforts, IMA CoD Left Low, IMA CoD Right Low, and IMA CoD Right Medium. These variables appear to be closely linked. We conclude that changes of direction, especially at lower and medium intensity levels, contribute to the overall player workload and explosive movement output. In contrast, Player Load Per Minute shows weaker correlations with other metrics, indicating that Total Player Load may be a more informative measure for analyzing physical demands.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 616}
key_metrics = df2[['Total Player Load', 'Player Load Per Minute', 'IMA Accel Total',
                   'IMA Decel Total', 'Explosive Efforts', 'Session Total Jump',
                   'Session Jumps Per Minute', 'Total High IMA', 'Total Acceleration Efforts']]
sns.heatmap(key_metrics.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Matrix for Key Metrics")
plt.show()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 472}
sns.boxplot(x='Position', y='Explosive Efforts', data=df2)
plt.title("Explosive Efforts by Position")
plt.xlabel("Position")
plt.ylabel("Explosive Efforts")
plt.show()
```

The boxplot above shows that median explosive efforts are relatively consistent across positions, indicating similar baseline intensity. However, guards and centers exhibit more high-end outliers, which suggests that some players in these roles may experience significantly greater explosive demands during certain sessions.

---

## Examining the relationship between player load and explosive efforts

Plotting Total Player Load vs Explosive Efforts as a scatterplot:

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 472}
sns.scatterplot(data=df1, x="Total Player Load", y="Explosive Efforts", color='blue')
sns.regplot(data=df1, x="Total Player Load", y="Explosive Efforts", scatter=False, color='red', line_kws={"linestyle":"dashed"})

plt.title("Total Player Load vs Explosive Efforts (Season 1 Data)")
plt.xlabel("Total Player Load")
plt.ylabel("# of Explosive Efforts")
plt.grid(True)
plt.show()
```

Crazy outlier at about (70, 162)! I want to look into this further.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 472}
sns.scatterplot(data=df2, x="Total Player Load", y="Explosive Efforts", color='red')
sns.regplot(data=df2, x="Total Player Load", y="Explosive Efforts", scatter=False, color='blue', line_kws={"linestyle":"dashed"})

plt.title("Total Player Load vs Explosive Efforts (Season 2 Data)")
plt.xlabel("Total Player Load")
plt.ylabel("# of Explosive Efforts")
plt.grid(True)
plt.show()
```

## Examining the Categorical Data

Let's examine the distribution of positions of all the players.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 872}
plt.figure(figsize=(15, 10))
sns.countplot(data=df1, x="Position", order=["Guard", "Forward", "Center"])
plt.title(f"Frequency Distribution of Players' Position in Season 1")
plt.xlabel("Position")
plt.ylabel("Count")
plt.show()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 872}
plt.figure(figsize=(15, 10))
sns.countplot(data=df2, x="Position", order=["Guard", "Forward", "Center"])
plt.title(f"Frequency Distribution of Players' Position in Season 2")
plt.xlabel("Position")
plt.ylabel("Count")
plt.show()
```

Clearly, there are less centers (or, more specifically, recorded data for centers, which I would assume means there are less people playing that position). What's interesting to me is that the number of entries for the Forward position drastically decreased in season 2. This makes me want to explore and compare other variables (in addition to position) season by season.

## Examining Player Load Per Minute

Let's analyze the variable Player Load Per Minute.

```{python}
#| colab: {base_uri: https://localhost:8080/}
variable = 'Player Load Per Minute'

df = pd.DataFrame({
    'Season 1': df1[variable],
    'Season 2': df2[variable]
})
print(df)
```

Referring back above, Season 2 has less entries, which is why there are missing values in our dataframe. Let's remove those entries and check for other missing values in hidden rows before we do a season by season comparison.

```{python}
#| colab: {base_uri: https://localhost:8080/}
df_cleaned = df.dropna(subset=['Season 2'])
print(df_cleaned)
```

```{python}
#| colab: {base_uri: https://localhost:8080/}
missing_values = df_cleaned.isnull().sum()
print(missing_values)
```

Perfect! We don't have any missing values anymore. Now we can create our visualizations. Here I want to use a box plot side by side comparison, a combined density plot, and a paired scatterplot.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 872}
df_melted = df[['Season 1','Season 2']].melt(
    var_name='Season',
    value_name='Player Load Per Minute')

plt.figure(figsize=(15, 10))
sns.boxplot(x='Season', y='Player Load Per Minute', data=df_melted)
plt.title('Comparison of Player Load Per Minute between Season 1 and Season 2')
plt.show()
```

This shows a slight difference in the median values between seasons, with Season 1 having a slightly higher PL/min than Season 2.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 872}
plt.figure(figsize=(15, 10))
sns.kdeplot(df['Season 1'], label='Season 1', fill=True, color='blue')
sns.kdeplot(df['Season 2'], label='Season 2', fill=True, color='red')
plt.title('Density Plot of Player Load Per Minute for Season 1 and Season 2')
plt.xlabel("Season")
plt.legend()
plt.show()
```

These distributions don't show too much differences between the seasons, there is significant overlap.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 872}
plt.figure(figsize=(15, 10))
plt.scatter(df['Season 1'], df['Season 2'], alpha=0.6, color='purple')
plt.title('Paired Scatterplot of Player Load Per Minute: Season 1 vs. Season 2')
plt.xlabel('Season 1')
plt.ylabel('Season 2')
plt.show()
```

This scatterplot above peaked our interest, because it seemed like most people who had relatively high player load per minute in season 1 had relatively low player load per minute in season 2 (and vice versa). We aren't sure why this is - maybe a higher player load per minute correlates to higher injury rates in the next season and a lower player load per minute correlates to being healthy and ready to go in the next season.

## Examining Variables Over Time

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 718}
plt.figure(figsize=(16, 8))

sns.lineplot(data=df1, x='Date', y='Player Load Per Minute', label='Season 1', color='blue', linewidth=2)

plt.title('Time Series of Player Load Per Minute Over Time')
plt.xlabel('Date')
plt.ylabel('Explosive Efforts')
plt.grid(True)
plt.legend()
plt.show()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 718}
plt.figure(figsize=(16, 8))

sns.lineplot(data=df1, x='Date', y='Total Player Load', label='Season 1', color='blue', linewidth=2)

plt.title('Time Series of Total Player Load Over Time')
plt.xlabel('Date')
plt.ylabel('Explosive Efforts')
plt.grid(True)
plt.legend()
plt.show()
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 718}
plt.figure(figsize=(16, 8))

# Plot Season 1
sns.lineplot(data=df1, x='Date', y='Explosive Efforts', label='Season 1', color='blue', linewidth=2)

plt.title('Time Series of Explosive Efforts Over Time')
plt.xlabel('Date')
plt.ylabel('Explosive Efforts')
plt.grid(True)
plt.legend()
plt.show()
```

Let's look at how explosive efforts have changed over the two seasons.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 564}
# Create a 'Season' column for each DataFrame
df1['Season'] = 1
df2['Season'] = 2

# Concatenate the DataFrames
df_combined = pd.concat([df1, df2])

# Convert 'Date' to datetime objects with error handling
df_combined['Date'] = pd.to_datetime(df_combined['Date'], format='%m/%d/%Y', errors='coerce')

# Extract week number from 'Date'
df_combined['Week'] = df_combined['Date'].dt.isocalendar().week

# Group by week and season, calculate mean of 'Explosive Efforts' and 'Player Load Per Minute'
weekly_stats = df_combined.groupby(['Season', 'Week'])[['Explosive Efforts', 'Player Load Per Minute']].mean().reset_index()

# Pivot the table for the heatmap
pivot_df = weekly_stats.pivot(index='Week', columns='Season', values='Explosive Efforts')

# Create the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(pivot_df, annot=True, cmap='coolwarm', fmt=".1f")
plt.title("Avg Explosive Efforts per Week by Season")
plt.xlabel("Season")
plt.ylabel("Week Number")
plt.show()
```

Looking at the heatmap for both seasons, it looks like explosive efforts have a tendency to increase initially, peak around the middle of the season, and then decrease/plateau towards the end. In season 1, the peak in explosive efforts is around weeks 6-8 and in season 2 the peak looks to be a little earlier in weeks 4-6. We also see a more noticeable decrease in explosive efforts towards the end of the season in comparison to season 1.

Now lets do the same with player load per minute.

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 564}
# Group by week and season, calculate mean of 'Explosive Efforts' and 'Player Load Per Minute'
weekly_stats = df_combined.groupby(['Season', 'Week'])[['Explosive Efforts', 'Player Load Per Minute']].mean().reset_index()

# Pivot the table for the heatmap
pivot_df = weekly_stats.pivot(index='Week', columns='Season', values='Player Load Per Minute')

# Create the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(pivot_df, annot=True, cmap='coolwarm', fmt=".1f")
plt.title("Avg Player Load Per Minute per Week by Season")
plt.xlabel("Season")
plt.ylabel("Week Number")
plt.show()
```

The training and competition schedule likely played a role in the patterns of player load per minute, with player workload decreasing in an attempt to prevent injuries and optimize performance.

## Possible Correlation
Our initial data analysis suggests a possible link between the patterns observed in player load and explosive efforts and the poorer performance UVA experienced in Season 2 compared to Season 1. The decline in explosive efforts and the inverse relationship seen in player load per minute shows a potential over-adjustment in workload management that affected performance in a negative way.

